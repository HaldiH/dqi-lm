{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from pydantic import PrivateAttr\n",
    "from typing import Any\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(generated_text):\n",
    "    \"\"\"\n",
    "    Extracts the first digit found in the generated text that is a valid class label.\n",
    "    \"\"\"\n",
    "    valid_digits = r\"\\d+\"\n",
    "    pattern = f\"[{valid_digits}]\"\n",
    "    match = re.search(pattern, generated_text)\n",
    "    if match:\n",
    "        return int(match.group(0))\n",
    "    else:\n",
    "        return -1  # Parsing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e42233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQIModel(weave.Model):\n",
    "    \"\"\"\n",
    "    Define an extra ChatModel class to store and version more parameters than just the model name.\n",
    "    This enables fine-tuning on specific parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    chat_model: str\n",
    "    cm_temperature: float\n",
    "    cm_max_new_tokens: int\n",
    "    cm_quantize: bool\n",
    "    inference_batch_size: int\n",
    "    dtype: Any\n",
    "    device: str\n",
    "    _model: Any = PrivateAttr()\n",
    "    _tokenizer: Any = PrivateAttr()\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        # unsloth version (enable native 2x faster inference)\n",
    "        self._model, self._tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=self.chat_model,\n",
    "            max_seq_length=self.cm_max_new_tokens,\n",
    "            dtype=self.dtype,\n",
    "            load_in_4bit=self.cm_quantize,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(self._model)\n",
    "\n",
    "    def predict(self, query: list[str]) -> dict:\n",
    "        # add_generation_prompt = true - Must add for generation\n",
    "        input_ids = self._tokenizer.apply_chat_template(\n",
    "            query,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        output_ids = self._model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=self.cm_max_new_tokens,\n",
    "            use_cache=True,\n",
    "            temperature=self.cm_temperature,\n",
    "            min_p=0.1,\n",
    "        )\n",
    "\n",
    "        decoded_outputs = self._tokenizer.batch_decode(\n",
    "            output_ids[0][input_ids.shape[1] :], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        generated_text = \"\".join(decoded_outputs).strip()\n",
    "        predicted_label = extract_score(generated_text)\n",
    "\n",
    "        return {\n",
    "            \"predicted_label\": predicted_label,\n",
    "            \"generated_text\": generated_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\n",
    "    \"json\", data_files=\"../data/processed_test.jsonl\", split=\"train\"\n",
    ").shuffle(seed=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = test_dataset[0]\n",
    "for message in example[\"messages\"]:\n",
    "    print(f\"{message['role']}:\")\n",
    "    print(f\"{message['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1bf8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = DQIModel(\n",
    "    chat_model=\"../outputs/mistral-7b-instruct-v0.3-bnb-4bit_finetuned\",\n",
    "    cm_temperature=0.1,\n",
    "    cm_max_new_tokens=256,\n",
    "    cm_quantize=True,\n",
    "    inference_batch_size=1,\n",
    "    dtype=\"float16\",\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 5\n",
    "for example in test_dataset.select(range(n_examples)):\n",
    "    query = example[\"messages\"][:-1]\n",
    "    y_true = example[\"messages\"][-1][\"content\"]\n",
    "    llm_response = llm.predict(query)\n",
    "    y_pred = extract_score(llm_response[\"generated_text\"])\n",
    "    print(f\"Ground truth: {y_true} | LLM prediction: {y_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqi-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
