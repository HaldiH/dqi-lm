{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88a9088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from pydantic import PrivateAttr\n",
    "from typing import Any\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe15b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(generated_text):\n",
    "    \"\"\"\n",
    "    Extracts the first digit found in the generated text that is a valid class label.\n",
    "    \"\"\"\n",
    "    valid_digits = r\"\\d+\"\n",
    "    pattern = f\"[{valid_digits}]\"\n",
    "    match = re.search(pattern, generated_text)\n",
    "    if match:\n",
    "        return int(match.group(0))\n",
    "    else:\n",
    "        return -1  # Parsing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e42233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQIModel:\n",
    "    \"\"\"\n",
    "    Define an extra ChatModel class to store and version more parameters than just the model name.\n",
    "    This enables fine-tuning on specific parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    chat_model: str\n",
    "    cm_temperature: float\n",
    "    cm_max_new_tokens: int\n",
    "    cm_quantize: bool\n",
    "    inference_batch_size: int\n",
    "    dtype: Any\n",
    "    device: str\n",
    "    _model: Any = PrivateAttr()\n",
    "    _tokenizer: Any = PrivateAttr()\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        # unsloth version (enable native 2x faster inference)\n",
    "        self._model, self._tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=self.chat_model,\n",
    "            max_seq_length=self.cm_max_new_tokens,\n",
    "            dtype=self.dtype,\n",
    "            load_in_4bit=self.cm_quantize,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(self._model)\n",
    "\n",
    "    async def predict(self, query: list[str]) -> dict:\n",
    "        # add_generation_prompt = true - Must add for generation\n",
    "        input_ids = self._tokenizer.apply_chat_template(\n",
    "            query,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        output_ids = self._model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=self.cm_max_new_tokens,\n",
    "            use_cache=True,\n",
    "            temperature=self.cm_temperature,\n",
    "            min_p=0.1,\n",
    "        )\n",
    "\n",
    "        decoded_outputs = self._tokenizer.batch_decode(\n",
    "            output_ids[0][input_ids.shape[1] :], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        generated_text = \"\".join(decoded_outputs).strip()\n",
    "        predicted_label = extract_score(generated_text)\n",
    "\n",
    "        return {\n",
    "            \"predicted_label\": predicted_label,\n",
    "            \"generated_text\": generated_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854d4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"json\", data_files=\"../data/processed_test.jsonl\", split=\"train\").shuffle(seed=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba0c25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system:\n",
      "### Role\n",
      "You are an expert in political discourse analysis using the Discourse Quality Index (DQI).\n",
      "\n",
      "### Task\n",
      "Evaluate the \"Level of Justification\" of the following text on a scale from 0 to 4.\n",
      "\n",
      "### Criteria Definitions\n",
      "* **0 (No justification):** The speaker demands that X should or should not be done, but provides NO reason.\n",
      "\n",
      "* **1 (Inferior justification):** A reason Y is given for demand X, but the logical connection between X and Y is missing or incomplete. It is a conclusion supported merely by illustrations or loose associations.\n",
      "\n",
      "* **2 (Qualified justification):** A single complete logical inference is made. The speaker explicitly explains WHY or HOW X contributes to Y (e.g., using connectors like \"because\", \"so that\").\n",
      "\n",
      "* **3 (Sophisticated justification - Broad):** The speaker provides at least two COMPLETE but DISTINCT justifications (Level 2) for the demand.\n",
      "*Structure:* \"We should do X because of Reason A. Additionally, we should do X because of Reason B.\" (Horizontal argumentation).\n",
      "\n",
      "* **4 (Sophisticated justification - In Depth):** The speaker provides multiple justifications where at least one contains an **embedded inference chain**. This means a reason is given, and that reason is itself further justified or connected to a deeper consequence.\n",
      "*Structure:* \"We should do X because it leads to Y, which in turn causes Z.\" (Vertical/Chained argumentation).\n",
      "\n",
      "### Instruction\n",
      "Analyze the text below and assign the correct score (0, 1, 2, 3, or 4). Return ONLY the integer.\n",
      "\n",
      "user:\n",
      "Text to analyse: 'Mr. President. if my colleague from Missouri would yield. it is my understanding and intention that Missouri would be exempt from the Brady 7day waiting period. S. 1241 exempts handgun transfers from the 7day waiting period if the law of the State in which the transfer occurs provides that a handgun transferee must have a permit to possess a handgun and that the permit is to be issued only after an authorized government official of the State has verified that the information available to such official does not indicate that possession of a handgun by the transferee would be in violation of law. Because Missouri law states that the sheriff shall issue a permit to acquire a concealable firearm only if the requirements of the Missouri permit law are met. the issuance of such a permit would itself be a verification that the information available to the sheriff does not indicate that possession of a handgun by the transferee would be in violation of law. Therefore. handgun purchases in Missouri would be exempt from the 7day waiting period.'\n",
      "\n",
      "assistant:\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = test_dataset[0]\n",
    "for message in example[\"messages\"]:\n",
    "    print(f\"{message['role']}:\")\n",
    "    print(f\"{message['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = DQIModel(\n",
    "    chat_model=\"outputs/mistral-7b-instruct-v0.3-bnb-4bit_finetuned\",\n",
    "    cm_temperature=0.1,\n",
    "    cm_max_new_tokens=256,\n",
    "    cm_quantize=True,\n",
    "    inference_batch_size=1,\n",
    "    dtype=\"float16\",\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "n_examples = 5\n",
    "for example in test_dataset.select(range(n_examples)):\n",
    "    query = [\n",
    "        (message[\"role\"], message[\"content\"]) for message in example[\"messages\"][:-1]\n",
    "    ]\n",
    "    y_true = example[\"messages\"][-1][\"content\"]\n",
    "    llm_response = llm.predict(query)\n",
    "    y_pred = extract_score(llm_response[\"generated_text\"])\n",
    "    print(f\"Ground truth: {y_true} | LLM prediction: {y_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqi-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
